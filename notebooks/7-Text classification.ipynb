{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"livereveal":{"scroll":true,"start_slideshow_at":"selected","transition":"none"},"colab":{"name":"Text classification.ipynb","provenance":[],"collapsed_sections":["WQz-RT3bVz5F","RuxrcSFUVz5G"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"MQlGHpdfVz4f"},"source":["# Text classification"]},{"cell_type":"markdown","metadata":{"id":"0JjcI3COVz4f"},"source":["## *\"Words. I know words. I have the best words!\"*\n","*- Noam Chomsky*"]},{"cell_type":"markdown","metadata":{"id":"9VwIn2GUVz4g"},"source":["# Overview\n","\n","In order to train a machine learning model to classify text, we need:\n","1. a way to preprocess text\n","2. a label for each text, represented as number\n","3. a way to represent each text as vector input\n","4. a model to learn  a function $f(input) = label$\n","5. a way to evaluate how well the model works\n","6. a way to predict new data"]},{"cell_type":"markdown","metadata":{"id":"l6rJC_TFVz4g"},"source":["As an example, we will use reviews data and try to classify the rating into $positive$ or $negative$, only based on the text they use.\n","\n","The same method can be used for any other data, including more labels and other dependent variables (e.g., age or gender of the text author, social constructs expressed in the text, etc...). "]},{"cell_type":"markdown","metadata":{"id":"TvC3_gQNVz4h"},"source":["# 1. Data"]},{"cell_type":"code","metadata":{"id":"K4ccSH_kVz4h","colab":{"base_uri":"https://localhost:8080/","height":128},"executionInfo":{"status":"ok","timestamp":1617892355686,"user_tz":-120,"elapsed":627,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"31dc5868-6077-4f17-bbf3-63b86b10d9cd"},"source":["import pandas as pd\n","\n","data = pd.read_csv('sa_train.csv', quoting=0)\n","print(len(data), data['output'].unique())\n","data.head(2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1800 ['neg' 'pos']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>shakespeare in love is quite possibly the most...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wizards is an animated feature that begins wit...</td>\n","      <td>neg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               input output\n","0  shakespeare in love is quite possibly the most...    neg\n","1  wizards is an animated feature that begins wit...    neg"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"cKX5AnLu7LJW","executionInfo":{"status":"ok","timestamp":1617892797841,"user_tz":-120,"elapsed":654,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"48212f8f-393f-4ccb-c428-f40469e52177"},"source":["data.iloc[0].input"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'shakespeare in love is quite possibly the most enjoyable period piece ever made for the silver screen . it is both humorous and romantic in a very unique blend that can successfully entertain any audience for the nearly 2 and and a half hours that it occupies . that is , however , not to say it is a good film , a quality production or anything of the sort . shakespeare in love is an incredibly cheap illusion that truly pans out to be very little quality or original work . the finest sign of this may be the plot , in looking back , there seems to be little more than a thin , predictable plot that is only carried by the portrayal of people that we revere in our history books . philip henslowe ( geoffrey rush ) owns 1 of the 2 theatres in london . it is at the peak of the royal theatre era , and queen elizabeth ( the recently damed judi dench , by , appropriately enough , queen elizabeth ii ) is very much a fan . however , to directly quote the film , he has \" cash flow problems . \" through a long set of events , it becomes apparent that his entire life is dependent on his next show doing well enough to pay off his debts . so , mr . henslowe employs the young playwright , william shakespeare ( joseph fiennes ) to pen a comedic production . however , the young writer has a severe case of writer \\' s block , and blames it on the fact that his love life is struggling as well . he has the title in mind , romeo and ethel , the pirate \\' s daughter ( even that joke loses steam after a while ) but can \\' t seem to put words to paper . then , as only hollywood could have it , through a long set of twisted events , he meets viola de lesseps ( gwyneth paltrow ) and falls madly in love , thus curing his writer \\' s block . there are many other little issues that mr . henslowe encounters , but they all pan out to be much ado about nothing . the first realization that i reached in watching this film is that one of the messages given is that a show should not always be credited to it \\' s author . ironically , that couldn \\' t be truer here . the great scenes that will sweep audiences away are not the scenes that fit in the plot , but rather the recitals of shakespearean lines by actors playing actors . one of the most breathtaking moments in this film does not involve the character of shakespeare or queen elizabeth or even the theatre owner , but rather 2 young children named romeo and juliet who chose to end their own lives in the name of love . so it is that i am offended by the fact that marc norman and tom stoppard are credited with writing this production , and the name william shakespeare is no were to be seen beyond a character \\' s name in the credits . the acting in this entertaining yet poor film is often thin to the point that it would not have survived even in queen elizabeth \\' s theatres . joseph fiennes may just be the worst of fall though . he is tragically unbelievable and comically bad . gwyneth paltrow is little more than satisfactory in her lead position as well . however , the supporting cast does almost save the day . geoffrey rush is nothing short of incredible and judi dench is breathtaking . they both seem to have shown that as proven actors they could survive in this film of weak links . you will also find a very good performance by ben affleck in his first real role since good will hunting ( no , armageddon doesn \\' t qualify as real acting . ) . and rupert everett was cute in his small part as well . but not even they could save this sad excuse for a film , so it remains plagued by poor performances . when all is said and done , shakespeare in love is only worth the trip if you want to be entertained . however , as the film so kindly pointed out , entertainment may be fun , but it isn \\' t necessarily quality . and this certainly isn \\' t quality . perhaps this may be best compared to a john grisham novel , as a dear friend of mine often does compare things to his work . simply put , it is far - fetched , poorly crafted , but very entertaining .'"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"DVEZ3uOh7wwY","executionInfo":{"status":"ok","timestamp":1617892942664,"user_tz":-120,"elapsed":510,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"406282dd-3471-4850-f66b-5048c9da5206"},"source":["data.iloc[0].clean_text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'shakespeare love quite possibly most enjoyable period piece ever make silver screen humorous romantic very unique blend can successfully entertain audience nearly half hour occupy that is however say good film quality production sort shakespeare love incredibly cheap illusion truly pan very little quality original work fine sign may plot look back seem little more thin predictable plot only carry portrayal people revere history book philip henslowe geoffrey rush own theatre london peak royal theatre era queen elizabeth recently dam judi dench by appropriately enough queen elizabeth ii very much fan however directly quote film cash flow problem long set event become apparent entire life dependent next show do well enough pay debt so mr . henslowe employ young playwright william shakespeare joseph fiennes pen comedic production however young writer severe case writer s block blame fact love life struggle as well title mind romeo ethel pirate daughter even joke lose steam while can t seem put word paper then only hollywood could long set twist event meet viola de lesseps gwyneth paltrow fall madly love thus cure writer s block many other little issue mr . henslowe encounter pan much ado first realization reach watch film message give show should always credit author ironically couldn t true here great scene will sweep audience away scene fit plot rather recital shakespearean line actor play actor most breathtaking moment film involve character shakespeare queen elizabeth even theatre owner rather young child name romeo juliet choose end own life name love so offend fact marc norman tom stoppard credit write production name william shakespeare see character name credit acting entertaining poor film often thin point would survive even queen elizabeth s theatre joseph fiennes may just bad fall though tragically unbelievable comically bad gwyneth paltrow little more satisfactory lead position as well however support cast almost save day geoffrey rush short incredible judi dench breathtake seem show prove actor could survive film weak link will also find very good performance ben affleck first real role good will hunt armageddon doesn t qualify real acting rupert everett cute small part as well even could save sad excuse film remain plague poor performance when say do shakespeare love only worth trip want entertain however film so kindly point entertainment may fun isn t necessarily quality certainly t quality perhaps may best compare john grisham novel dear friend often compare thing work simply put far fetched poorly craft very entertaining'"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"aq9uSkU2Vz4k"},"source":["## Preprocessing\n","\n","Text is messy. The goal of preprocessing is to reduce the amount of noise (= unnecessary variation), while maintaining the signal. There is no one-size-fits-all solution, but a good approximation is the following:"]},{"cell_type":"code","metadata":{"id":"JzWUUADMVz4k"},"source":["import spacy\n","nlp = spacy.load('en', disable=['parser', 'ner'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3qjwErbyVz4n","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1617892359048,"user_tz":-120,"elapsed":1056,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"08796fb8-198f-43e9-bd84-8871c7375832"},"source":["def clean_text(text):\n","    '''reduce text to lower-case lexicon entry'''\n","    lemmas = [token.lemma_ for token in nlp(text) \n","              if token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN'}]\n","    return ' '.join(lemmas)\n","\n","clean_text('This is a test sentence. And here comes another one... Go me!')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'test sentence here come one go'"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"FZ2lm8GGVz4p"},"source":["Let's clean up the input data. This can take a while, so it's good to save it."]},{"cell_type":"code","metadata":{"id":"fxdCFYKRVz4p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617892419240,"user_tz":-120,"elapsed":60496,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"bd73869e-cf52-468e-f4f9-749b294daa08"},"source":["data['clean_text'] = data['input'].apply(clean_text)\n","data['clean_text'].head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    shakespeare love quite possibly most enjoyable...\n","1    wizard animate feature begin narration epic pr...\n","2    gun wielding arnold schwarzenegger change hear...\n","3    keep jane austen sense sensibility pride preju...\n","4    hollywood pimp fat cigar smoking chump wear fu...\n","Name: clean_text, dtype: object"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"YmrTaAOdiHZJ","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1617893049087,"user_tz":-120,"elapsed":655,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"d69a0167-e6cd-4c76-fc7c-07c3ddaf12bd"},"source":["data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>output</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>shakespeare in love is quite possibly the most...</td>\n","      <td>neg</td>\n","      <td>shakespeare love quite possibly most enjoyable...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wizards is an animated feature that begins wit...</td>\n","      <td>neg</td>\n","      <td>wizard animate feature begin narration epic pr...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>gun wielding arnold schwarzenegger has a chang...</td>\n","      <td>neg</td>\n","      <td>gun wielding arnold schwarzenegger change hear...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>if this keeps up , jane austen ( sense and sen...</td>\n","      <td>pos</td>\n","      <td>keep jane austen sense sensibility pride preju...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hollywood is a pimp . a fat , cigar - smoking ...</td>\n","      <td>pos</td>\n","      <td>hollywood pimp fat cigar smoking chump wear fu...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               input  ...                                         clean_text\n","0  shakespeare in love is quite possibly the most...  ...  shakespeare love quite possibly most enjoyable...\n","1  wizards is an animated feature that begins wit...  ...  wizard animate feature begin narration epic pr...\n","2  gun wielding arnold schwarzenegger has a chang...  ...  gun wielding arnold schwarzenegger change hear...\n","3  if this keeps up , jane austen ( sense and sen...  ...  keep jane austen sense sensibility pride preju...\n","4  hollywood is a pimp . a fat , cigar - smoking ...  ...  hollywood pimp fat cigar smoking chump wear fu...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"cz6B9HrzVz4r"},"source":["# 2. Labels\n","\n","Here, we assume that we already have the labels. (In your task, you will have to label them yourself! Hint: use `input()` or a spreadsheet).\n","\n","However, in order for the machine learning model to work with the labels, we need to translate them into a vector of numbers. We can use `sklearn.LabelEncoder`"]},{"cell_type":"code","metadata":{"id":"iC0Z9QLCVz4s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617893063224,"user_tz":-120,"elapsed":586,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"d0b3bfa8-db9c-407f-c98b-dfc664b9f30c"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# transform labels into numbers\n","labels2numbers = LabelEncoder()\n","\n","y = labels2numbers.fit_transform(data['output'])\n","print(data['output'][:10], y[:10], len(y))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0    neg\n","1    neg\n","2    neg\n","3    pos\n","4    pos\n","5    neg\n","6    pos\n","7    pos\n","8    neg\n","9    neg\n","Name: output, dtype: object [0 0 0 1 1 0 1 1 0 0] 1800\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ce3RqPSnVz4u"},"source":["To get the original names back, use `inverse_transform()`:"]},{"cell_type":"code","metadata":{"id":"jWUAa5qRVz4u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617893125825,"user_tz":-120,"elapsed":516,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"9ca645b3-4865-44a5-9542-23308168496f"},"source":["labels2numbers.inverse_transform([1,1,1,0,0,1])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['pos', 'pos', 'pos', 'neg', 'neg', 'pos'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"B1ji_rubVz4x"},"source":["# 3. Representing text\n","\n","First, we need to transform the texts into a matrix, where each row represents one text instance. The columns are the **features**\n"]},{"cell_type":"code","metadata":{"id":"ui4ie7e6Vz4x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617893324703,"user_tz":-120,"elapsed":3996,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"08605a6a-f8a8-436d-a505-b3603396e64f"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(ngram_range=(1,2), \n","                             min_df=0.001, \n","                             max_df=0.75, \n","                             stop_words='english')\n","\n","X = vectorizer.fit_transform(data['clean_text'])\n","print(X.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1800, 66808)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MqzzmzwsVz40"},"source":["We can now translate back and forth between columns and words:"]},{"cell_type":"code","metadata":{"id":"92xme8BZVz40","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617893370151,"user_tz":-120,"elapsed":515,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"3d8a1021-1198-4679-f283-d0185510a7ee"},"source":["vectorizer.vocabulary_['bad']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3786"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"X3GJ324yVz42","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1617893371592,"user_tz":-120,"elapsed":664,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"288a36c0-6c7b-449d-8b05-51c3afde2ac1"},"source":["vectorizer.get_feature_names()[3786]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'bad'"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"hXZ-Xw5yVz44"},"source":["Let's see how often that word is in the data:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"5RWP8Xb99olE","executionInfo":{"status":"ok","timestamp":1617893450082,"user_tz":-120,"elapsed":640,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"93e87dbf-8d4b-4aff-d381-f83a6db8da98"},"source":["data[data.clean_text.str.contains(' bad ')]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>output</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>shakespeare in love is quite possibly the most...</td>\n","      <td>neg</td>\n","      <td>shakespeare love quite possibly most enjoyable...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wizards is an animated feature that begins wit...</td>\n","      <td>neg</td>\n","      <td>wizard animate feature begin narration epic pr...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hollywood is a pimp . a fat , cigar - smoking ...</td>\n","      <td>pos</td>\n","      <td>hollywood pimp fat cigar smoking chump wear fu...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>films adapted from comic books have had plenty...</td>\n","      <td>pos</td>\n","      <td>film adapt comic book have plenty success supe...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>to watch ` battlefield earth ' is to wallow in...</td>\n","      <td>neg</td>\n","      <td>watch battlefield earth wallow misery most lud...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1794</th>\n","      <td>ladies and gentlemen , 1997 ' s independence d...</td>\n","      <td>pos</td>\n","      <td>lady gentleman s independence day here title s...</td>\n","    </tr>\n","    <tr>\n","      <th>1795</th>\n","      <td>terrence malick made an excellent 90 minute fi...</td>\n","      <td>neg</td>\n","      <td>terrence malick make excellent minute film ada...</td>\n","    </tr>\n","    <tr>\n","      <th>1796</th>\n","      <td>as you should know , this summer has been less...</td>\n","      <td>neg</td>\n","      <td>should know summer less memorable total decent...</td>\n","    </tr>\n","    <tr>\n","      <th>1798</th>\n","      <td>a movie about divorce and custody in 1995 seem...</td>\n","      <td>neg</td>\n","      <td>movie divorce custody seem about as timely mov...</td>\n","    </tr>\n","    <tr>\n","      <th>1799</th>\n","      <td>plot : a down - and - out girl moves in with s...</td>\n","      <td>neg</td>\n","      <td>plot down girl move top model fall love goofy ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>857 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                  input  ...                                         clean_text\n","0     shakespeare in love is quite possibly the most...  ...  shakespeare love quite possibly most enjoyable...\n","1     wizards is an animated feature that begins wit...  ...  wizard animate feature begin narration epic pr...\n","4     hollywood is a pimp . a fat , cigar - smoking ...  ...  hollywood pimp fat cigar smoking chump wear fu...\n","6     films adapted from comic books have had plenty...  ...  film adapt comic book have plenty success supe...\n","8     to watch ` battlefield earth ' is to wallow in...  ...  watch battlefield earth wallow misery most lud...\n","...                                                 ...  ...                                                ...\n","1794  ladies and gentlemen , 1997 ' s independence d...  ...  lady gentleman s independence day here title s...\n","1795  terrence malick made an excellent 90 minute fi...  ...  terrence malick make excellent minute film ada...\n","1796  as you should know , this summer has been less...  ...  should know summer less memorable total decent...\n","1798  a movie about divorce and custody in 1995 seem...  ...  movie divorce custody seem about as timely mov...\n","1799  plot : a down - and - out girl moves in with s...  ...  plot down girl move top model fall love goofy ...\n","\n","[857 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"Y85dIjYsVz45","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617893418418,"user_tz":-120,"elapsed":754,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"a7e87d61-c41b-4b64-89af-2af4497da326"},"source":["len(data[data.clean_text.str.contains('bad')])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["895"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"jdqEPjjKVz47"},"source":["# 4. Learning a classification model\n","\n","A classification model is simply a function that takes a text representation as input, and returns an output label.\n","\n","Inside that function is normally a set of weights. By multiplying the weight vector with the input vector, we get the label."]},{"cell_type":"markdown","metadata":{"id":"iQ1Nw6wGVz47"},"source":["## 4.1: Fitting a model\n","\n","Fitting a model is the process of finding the right weights to map the training inputs to the training outputs. Fitting to data in `sklearn` is easy: we use the `fit()` function, giving it the input matrix and output vector."]},{"cell_type":"code","metadata":{"id":"ObVUO3JqVz48","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617893535909,"user_tz":-120,"elapsed":2177,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"c8b86a04-01c9-4aa2-b9b0-ceacfaeb0a4e"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","classifier = LogisticRegression(n_jobs=-1, class_weight='balanced')\n","%time classifier.fit(X, y)\n","print(classifier)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 31.7 ms, sys: 81.9 ms, total: 114 ms\n","Wall time: 1.53 s\n","LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n","                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n","                   max_iter=100, multi_class='auto', n_jobs=-1, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nr_RwOdtVz4-"},"source":["The resulting fitted model has coefficients (betas) for each word/feature in our vocabulary"]},{"cell_type":"code","metadata":{"id":"u-BCqLYrVz4-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617893582584,"user_tz":-120,"elapsed":535,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"9ee34b1a-18e3-40e4-c55e-9330bf7ac8de"},"source":["coefs = classifier.coef_\n","coefs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.00986428, -0.06329059, -0.03779013, ...,  0.06591883,\n","         0.02520406, -0.00039513]])"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"pmDSYjKGVz5B"},"source":["We can now examine the weights/coefficients/betas for the individual words (note that each word has an ID):"]},{"cell_type":"code","metadata":{"id":"1yHdzoH5Vz5C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617893602589,"user_tz":-120,"elapsed":765,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"944b86d9-965b-43ee-c443-96758a2f8bbb"},"source":["k = vectorizer.vocabulary_['bad'] # column position for the word\n","print(vectorizer.get_feature_names()[k], classifier.coef_[0, k])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bad -3.493747454624923\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KACMKKLu-ZEO","executionInfo":{"status":"ok","timestamp":1617893630095,"user_tz":-120,"elapsed":503,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"02d7722b-d464-49a5-c2ca-7c780326b397"},"source":["k = vectorizer.vocabulary_['good'] # column position for the word\n","print(vectorizer.get_feature_names()[k], classifier.coef_[0, k])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["good 0.8523441444658751\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"itvwkAATVz5E"},"source":["NB: in a two-class problem, our coefficents are in a vector: positive values indicate the positive class, negative values the other class.\n","In a multi-class problem, we have one **row** of coefficients for each class: positive values indicate that this feature contributes to the class, negative values indicate that it contributes to other classes."]},{"cell_type":"markdown","metadata":{"id":"vlX7iByXVz5F"},"source":["# 5. Evaluating models\n","\n","Having a model is great, but how well does it do? Can it classify what it has seen? We need a way to estimate how well the model will work on new data.\n","\n","We need a metric to measure performance and a way to simulate new data."]},{"cell_type":"markdown","metadata":{"id":"8Q0Wg78sVz5F"},"source":["## 5.1: Metrics\n","\n","We use three measure:\n","1. precision\n","2. recall\n","3. F1"]},{"cell_type":"markdown","metadata":{"id":"WQz-RT3bVz5F"},"source":["### Precision\n","\n","Precision measures how many of our model's predictions were correct. We divide the number of true positives by the number of all positives\n","\n","$$\n","p = \\frac{tp}{tp+fp}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"RuxrcSFUVz5G"},"source":["### Recall\n","\n","Recall measures how many of the correct answers in the data our model managed to find. We divide the number of true positives by the number of true positives (the instances our model got) and false negatives (the instances our model *should* have gotten)\n","\n","$$\n","r = \\frac{tp}{tp+fn}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Au9AhOaUVz5G"},"source":["### F1\n","\n","A model that classified everything as, say, \"positive\" would get a perfect recall (it does, after all, find all positive examples). However, such a model would obviously be useless, since its precision is bad.\n","\n","We want to balance the two against each other. F1 does exactly that, by taking the harmonic mean.\n","\n","$$\n","F_1 = \\frac{p\\cdot r}{p+r}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"JZg3zpb9Vz5H"},"source":["Luckily, all of these metrics are implemented in `sklearn`. All we have to provide are the predictions of our model, and the actual correct answers (called the *gold standard*). "]},{"cell_type":"code","metadata":{"id":"Lc_wtcu9Vz5H"},"source":["from sklearn.metrics import classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NeQ8FWL5Vz5K"},"source":["## 5.2: Cross-validation\n","\n","How do we measure performance on new data, if we don't know what the correct outputs for those new data points are?\n","\n","In **$k$-fold cross-validation**, we simulate new data, by fitting our model on parts of the data, and evaluating on other. We can thereby measure the performance on the held-out part. "]},{"cell_type":"markdown","metadata":{"id":"DoLWzhNDVz5K"},"source":["However, we have now reduced the amount of data we used to fit the data. In order to address this, we simply repeat the process $k$ times.\n","We separate the data into $k$ parts, fit the model on $k-1$ parts, and evaluate on the $k$th part. In the end, we have performance scores from $k$ models. The average of them tells us how well the model would work on new data.\n","\n"]},{"cell_type":"code","metadata":{"id":"GjyLdz_gVz5L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617894010763,"user_tz":-120,"elapsed":7617,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"a87b61b1-7308-4e9b-bdb1-c7b56e589f8e"},"source":["from sklearn.model_selection import cross_val_score\n","\n","for k in [2,3,5,10]:\n","    cv = cross_val_score(LogisticRegression(), X, y=y, cv=k, n_jobs=-1, scoring=\"f1_micro\")\n","    fold_size = X.shape[0]/k\n","    \n","    print(\"F1 with {} folds for bag-of-words is {}\".format(k, cv.mean()))\n","    print(\"Training on {} instances/fold, testing on {}\".format(fold_size*(k-1), fold_size))\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["F1 with 2 folds for bag-of-words is 0.8083333333333333\n","Training on 900.0 instances/fold, testing on 900.0\n","\n","F1 with 3 folds for bag-of-words is 0.8172222222222222\n","Training on 1200.0 instances/fold, testing on 600.0\n","\n","F1 with 5 folds for bag-of-words is 0.828888888888889\n","Training on 1440.0 instances/fold, testing on 360.0\n","\n","F1 with 10 folds for bag-of-words is 0.8305555555555555\n","Training on 1620.0 instances/fold, testing on 180.0\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M_XkJ3CgVz5N"},"source":["## Baselines\n","So, is that performance good? Let's compare to a **baseline**, i.e., a null-hypothesis. The simplest one is that all instances belong to the most frequnt class in the data."]},{"cell_type":"code","metadata":{"id":"8fvRRMb3Vz5N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617894306372,"user_tz":-120,"elapsed":573,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"23088dcc-640d-462e-e263-aaecb36acc33"},"source":["from sklearn.dummy import DummyClassifier\n","\n","most_frequent = DummyClassifier(strategy='most_frequent')\n","\n","print(cross_val_score(most_frequent, X, y=y, cv=5, n_jobs=-1, scoring=\"f1_micro\").mean())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.5061111111111111\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t4cEIoxVVz5P"},"source":["# Exercise\n","\n","See whether you can apply the previous steps to a new data sets, a description of wines. Choose any of the descriptor columns as target variable. The text is already preprocessed, to save time."]},{"cell_type":"code","metadata":{"id":"-DgVNPbgVz5P","colab":{"base_uri":"https://localhost:8080/","height":725},"executionInfo":{"status":"ok","timestamp":1617894439304,"user_tz":-120,"elapsed":3090,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"689101cc-2e2f-4e26-ba04-cd78aebe0d15"},"source":["wine = pd.read_excel('wine_reviews_small.xlsx', nrows=10000)\n","wine"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>country</th>\n","      <th>description</th>\n","      <th>designation</th>\n","      <th>points</th>\n","      <th>price</th>\n","      <th>province</th>\n","      <th>region_1</th>\n","      <th>region_2</th>\n","      <th>variety</th>\n","      <th>winery</th>\n","      <th>description_cleaned</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>US</td>\n","      <td>This tremendous 100% varietal wine hails from ...</td>\n","      <td>Martha's Vineyard</td>\n","      <td>96</td>\n","      <td>235.0</td>\n","      <td>California</td>\n","      <td>Napa Valley</td>\n","      <td>Napa</td>\n","      <td>Cabernet Sauvignon</td>\n","      <td>Heitz</td>\n","      <td>tremendous varietal wine hail be age year oak ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Spain</td>\n","      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n","      <td>Carodorum Selección Especial Reserva</td>\n","      <td>96</td>\n","      <td>110.0</td>\n","      <td>Northern Spain</td>\n","      <td>Toro</td>\n","      <td>NaN</td>\n","      <td>Tinta de Toro</td>\n","      <td>Bodega Carmen Rodríguez</td>\n","      <td>ripe aroma fig blackberry cassis be soften swe...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>US</td>\n","      <td>Mac Watson honors the memory of a wine once ma...</td>\n","      <td>Special Selected Late Harvest</td>\n","      <td>96</td>\n","      <td>90.0</td>\n","      <td>California</td>\n","      <td>Knights Valley</td>\n","      <td>Sonoma</td>\n","      <td>Sauvignon Blanc</td>\n","      <td>Macauley</td>\n","      <td>honor memory wine once make his mother tremend...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>US</td>\n","      <td>This spent 20 months in 30% new French oak, an...</td>\n","      <td>Reserve</td>\n","      <td>96</td>\n","      <td>65.0</td>\n","      <td>Oregon</td>\n","      <td>Willamette Valley</td>\n","      <td>Willamette Valley</td>\n","      <td>Pinot Noir</td>\n","      <td>Ponzi</td>\n","      <td>spend month new french oak incorporate fruit v...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>France</td>\n","      <td>This is the top wine from La Bégude, named aft...</td>\n","      <td>La Brûlade</td>\n","      <td>95</td>\n","      <td>66.0</td>\n","      <td>Provence</td>\n","      <td>Bandol</td>\n","      <td>NaN</td>\n","      <td>Provence red blend</td>\n","      <td>Domaine de la Bégude</td>\n","      <td>be top wine name high point vineyard foot have...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>9995</td>\n","      <td>9997</td>\n","      <td>Chile</td>\n","      <td>Leesy vanilla aromas are bland and don't amoun...</td>\n","      <td>NaN</td>\n","      <td>81</td>\n","      <td>13.0</td>\n","      <td>Central Valley</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Sauvignon Blanc</td>\n","      <td>Kon Tiki</td>\n","      <td>leesy vanilla aroma be bland do not amount muc...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>9996</td>\n","      <td>9998</td>\n","      <td>Chile</td>\n","      <td>Powdery bath-soap aromas are odd on the nose. ...</td>\n","      <td>Gran Reserva</td>\n","      <td>81</td>\n","      <td>11.0</td>\n","      <td>Leyda Valley</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Sauvignon Blanc</td>\n","      <td>Autoritas</td>\n","      <td>powdery bath soap aroma be odd nose mealy pala...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>9997</td>\n","      <td>9999</td>\n","      <td>Chile</td>\n","      <td>Aromas of spiced tomato, clove and rubber glov...</td>\n","      <td>Bold</td>\n","      <td>81</td>\n","      <td>10.0</td>\n","      <td>Central Valley</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Cabernet Sauvignon</td>\n","      <td>Terra Andina</td>\n","      <td>spice tomato clove rubber glove do not click s...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>9998</td>\n","      <td>10000</td>\n","      <td>Spain</td>\n","      <td>Right away this goes wrong; the nose is smothe...</td>\n","      <td>Vendimia Seleccionada</td>\n","      <td>80</td>\n","      <td>17.0</td>\n","      <td>Northern Spain</td>\n","      <td>Ribera del Duero</td>\n","      <td>NaN</td>\n","      <td>Tempranillo</td>\n","      <td>Sembro</td>\n","      <td>right away go wrong nose be smother char burn ...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>9999</td>\n","      <td>10001</td>\n","      <td>Portugal</td>\n","      <td>This is a light, fruity and off-dry wine. It h...</td>\n","      <td>NaN</td>\n","      <td>80</td>\n","      <td>8.0</td>\n","      <td>Vinho Verde</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Portuguese White</td>\n","      <td>Mapreco</td>\n","      <td>be light fruity off dry wine have splash lemon...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 13 columns</p>\n","</div>"],"text/plain":["      Unnamed: 0  ...                                description_cleaned\n","0              0  ...  tremendous varietal wine hail be age year oak ...\n","1              1  ...  ripe aroma fig blackberry cassis be soften swe...\n","2              2  ...  honor memory wine once make his mother tremend...\n","3              3  ...  spend month new french oak incorporate fruit v...\n","4              4  ...  be top wine name high point vineyard foot have...\n","...          ...  ...                                                ...\n","9995        9995  ...  leesy vanilla aroma be bland do not amount muc...\n","9996        9996  ...  powdery bath soap aroma be odd nose mealy pala...\n","9997        9997  ...  spice tomato clove rubber glove do not click s...\n","9998        9998  ...  right away go wrong nose be smother char burn ...\n","9999        9999  ...  be light fruity off dry wine have splash lemon...\n","\n","[10000 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"4lAdP0BWVz5R"},"source":["# your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_zUQtfnJVz5T"},"source":["# 6 Heldout data\n","\n","Classifying new (**held-out**) data is called **prediction**. We reuse the weights we have learned before on a new data matrix to predict the new outcomes.\n","Important: the new data needs to have the same number of features!"]},{"cell_type":"code","metadata":{"id":"POHdSR7EVz5U","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1617896635112,"user_tz":-120,"elapsed":492,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"70a42eca-b613-4cad-c07f-41bd0861f917"},"source":["# read in new data set\n","new_data = pd.read_csv('sa_test.csv')\n","print(len(new_data))\n","new_data.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["200\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>robert redford ' s a river runs through it is ...</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>if the 70 ' s nostalgia didn ' t make you feel...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>you think that these people only exist in the ...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\" knock off \" is exactly that : a cheap knock ...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>brian depalma needs a hit * really * badly . s...</td>\n","      <td>pos</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               input output\n","0  robert redford ' s a river runs through it is ...    pos\n","1  if the 70 ' s nostalgia didn ' t make you feel...    neg\n","2  you think that these people only exist in the ...    neg\n","3  \" knock off \" is exactly that : a cheap knock ...    neg\n","4  brian depalma needs a hit * really * badly . s...    pos"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"wS5DsydTVz5W"},"source":["Don't forget to clean it!"]},{"cell_type":"code","metadata":{"id":"6ZS4dScqVz5X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617896664443,"user_tz":-120,"elapsed":6416,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"df4bff43-9dea-4f9f-9ec0-6186e0d19b0a"},"source":["%time new_data['clean_text'] = new_data.input.apply(clean_text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 5.79 s, sys: 95.5 ms, total: 5.88 s\n","Wall time: 5.89 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VHJtIjPFVz5b"},"source":["Let's see how well we do on this data:"]},{"cell_type":"code","metadata":{"id":"lzNk3BuBVz5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617896686628,"user_tz":-120,"elapsed":514,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"1ac1a2ef-1a16-4838-c93e-f78ed27d6daf"},"source":["# transform text into word counts\n","# IMPORTANT: use same vectorizer we fit on training data to create vectors!\n","new_X = vectorizer.transform(new_data['clean_text'])\n","\n","# translate labels\n","new_y = labels2numbers.transform(new_data['output'])\n","\n","\n","# use the old classifier to predict and evaluate\n","new_predictions = classifier.predict(new_X)\n","print(new_predictions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1\n"," 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1\n"," 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0\n"," 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0\n"," 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0\n"," 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rC1QJpKqVz5d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617896696305,"user_tz":-120,"elapsed":503,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"4370ca9d-e07c-46b2-c44f-c1492b6eca51"},"source":["print(classification_report(new_y, new_predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.83      0.84      0.83       111\n","           1       0.80      0.79      0.79        89\n","\n","    accuracy                           0.81       200\n","   macro avg       0.81      0.81      0.81       200\n","weighted avg       0.81      0.81      0.81       200\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hGz7iIHoVz5f"},"source":["Instead, we can also predict the probabilities of belonging to each class"]},{"cell_type":"code","metadata":{"id":"JcDpM1w5Vz5f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617896754961,"user_tz":-120,"elapsed":509,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"13abc3b2-986b-42d5-a91d-94f085837760"},"source":["new_probabilities = classifier.predict_proba(new_X)\n","print(new_probabilities)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.30745711 0.69254289]\n"," [0.55629129 0.44370871]\n"," [0.5346982  0.4653018 ]\n"," [0.74681023 0.25318977]\n"," [0.36336666 0.63663334]\n"," [0.63447098 0.36552902]\n"," [0.52763496 0.47236504]\n"," [0.53041422 0.46958578]\n"," [0.62989898 0.37010102]\n"," [0.52398668 0.47601332]\n"," [0.39571583 0.60428417]\n"," [0.27701891 0.72298109]\n"," [0.36529535 0.63470465]\n"," [0.6180369  0.3819631 ]\n"," [0.37767803 0.62232197]\n"," [0.34803811 0.65196189]\n"," [0.30380553 0.69619447]\n"," [0.3705448  0.6294552 ]\n"," [0.50506119 0.49493881]\n"," [0.46744111 0.53255889]\n"," [0.71648016 0.28351984]\n"," [0.48074602 0.51925398]\n"," [0.34768242 0.65231758]\n"," [0.40778423 0.59221577]\n"," [0.51954813 0.48045187]\n"," [0.58126258 0.41873742]\n"," [0.69281727 0.30718273]\n"," [0.67484025 0.32515975]\n"," [0.46197409 0.53802591]\n"," [0.5516414  0.4483586 ]\n"," [0.29534742 0.70465258]\n"," [0.64829551 0.35170449]\n"," [0.71811139 0.28188861]\n"," [0.81374881 0.18625119]\n"," [0.66188463 0.33811537]\n"," [0.73474834 0.26525166]\n"," [0.30462163 0.69537837]\n"," [0.58988807 0.41011193]\n"," [0.59001609 0.40998391]\n"," [0.64077973 0.35922027]\n"," [0.67182359 0.32817641]\n"," [0.67460547 0.32539453]\n"," [0.73137522 0.26862478]\n"," [0.73638276 0.26361724]\n"," [0.38788685 0.61211315]\n"," [0.42429875 0.57570125]\n"," [0.70334106 0.29665894]\n"," [0.59688467 0.40311533]\n"," [0.66079787 0.33920213]\n"," [0.69147161 0.30852839]\n"," [0.36483898 0.63516102]\n"," [0.25696651 0.74303349]\n"," [0.63129232 0.36870768]\n"," [0.38832969 0.61167031]\n"," [0.51863917 0.48136083]\n"," [0.64399717 0.35600283]\n"," [0.49134407 0.50865593]\n"," [0.2893031  0.7106969 ]\n"," [0.61911911 0.38088089]\n"," [0.54946439 0.45053561]\n"," [0.71439695 0.28560305]\n"," [0.32096212 0.67903788]\n"," [0.59239414 0.40760586]\n"," [0.61342379 0.38657621]\n"," [0.37931455 0.62068545]\n"," [0.43214396 0.56785604]\n"," [0.54737737 0.45262263]\n"," [0.52649472 0.47350528]\n"," [0.59334677 0.40665323]\n"," [0.34985223 0.65014777]\n"," [0.50028786 0.49971214]\n"," [0.49062582 0.50937418]\n"," [0.66777229 0.33222771]\n"," [0.27403087 0.72596913]\n"," [0.46899063 0.53100937]\n"," [0.53833011 0.46166989]\n"," [0.49774504 0.50225496]\n"," [0.60129157 0.39870843]\n"," [0.61402559 0.38597441]\n"," [0.44803662 0.55196338]\n"," [0.37180692 0.62819308]\n"," [0.25657754 0.74342246]\n"," [0.40491646 0.59508354]\n"," [0.26517246 0.73482754]\n"," [0.44212763 0.55787237]\n"," [0.3725833  0.6274167 ]\n"," [0.64182984 0.35817016]\n"," [0.80632551 0.19367449]\n"," [0.51455199 0.48544801]\n"," [0.58042128 0.41957872]\n"," [0.57465969 0.42534031]\n"," [0.4958256  0.5041744 ]\n"," [0.48248777 0.51751223]\n"," [0.37235917 0.62764083]\n"," [0.29766449 0.70233551]\n"," [0.54467837 0.45532163]\n"," [0.65529801 0.34470199]\n"," [0.33047553 0.66952447]\n"," [0.60049558 0.39950442]\n"," [0.40650665 0.59349335]\n"," [0.38224084 0.61775916]\n"," [0.26606111 0.73393889]\n"," [0.55881275 0.44118725]\n"," [0.57504669 0.42495331]\n"," [0.53709358 0.46290642]\n"," [0.55515451 0.44484549]\n"," [0.31590584 0.68409416]\n"," [0.55881735 0.44118265]\n"," [0.52980026 0.47019974]\n"," [0.66756612 0.33243388]\n"," [0.56341302 0.43658698]\n"," [0.82208003 0.17791997]\n"," [0.52885354 0.47114646]\n"," [0.39835937 0.60164063]\n"," [0.38356057 0.61643943]\n"," [0.6651991  0.3348009 ]\n"," [0.49198794 0.50801206]\n"," [0.44690735 0.55309265]\n"," [0.54348459 0.45651541]\n"," [0.43468199 0.56531801]\n"," [0.6048523  0.3951477 ]\n"," [0.62209282 0.37790718]\n"," [0.33223595 0.66776405]\n"," [0.5022371  0.4977629 ]\n"," [0.38865194 0.61134806]\n"," [0.58249405 0.41750595]\n"," [0.53585587 0.46414413]\n"," [0.37018286 0.62981714]\n"," [0.47516366 0.52483634]\n"," [0.67247594 0.32752406]\n"," [0.46743095 0.53256905]\n"," [0.83379617 0.16620383]\n"," [0.79549812 0.20450188]\n"," [0.24402533 0.75597467]\n"," [0.34168976 0.65831024]\n"," [0.28680251 0.71319749]\n"," [0.43766946 0.56233054]\n"," [0.4311006  0.5688994 ]\n"," [0.58495817 0.41504183]\n"," [0.55233648 0.44766352]\n"," [0.65441504 0.34558496]\n"," [0.63449288 0.36550712]\n"," [0.66110843 0.33889157]\n"," [0.54204271 0.45795729]\n"," [0.47811378 0.52188622]\n"," [0.2290934  0.7709066 ]\n"," [0.56317447 0.43682553]\n"," [0.66850296 0.33149704]\n"," [0.38425935 0.61574065]\n"," [0.43506152 0.56493848]\n"," [0.38085177 0.61914823]\n"," [0.50475642 0.49524358]\n"," [0.37782611 0.62217389]\n"," [0.41914351 0.58085649]\n"," [0.66971212 0.33028788]\n"," [0.25785411 0.74214589]\n"," [0.6315107  0.3684893 ]\n"," [0.45123209 0.54876791]\n"," [0.35668661 0.64331339]\n"," [0.62740761 0.37259239]\n"," [0.53911846 0.46088154]\n"," [0.37266271 0.62733729]\n"," [0.27386495 0.72613505]\n"," [0.51260269 0.48739731]\n"," [0.67826453 0.32173547]\n"," [0.44976033 0.55023967]\n"," [0.59952272 0.40047728]\n"," [0.75514273 0.24485727]\n"," [0.60752964 0.39247036]\n"," [0.51004392 0.48995608]\n"," [0.56792876 0.43207124]\n"," [0.51691449 0.48308551]\n"," [0.32476754 0.67523246]\n"," [0.37411754 0.62588246]\n"," [0.38791725 0.61208275]\n"," [0.69338943 0.30661057]\n"," [0.50491139 0.49508861]\n"," [0.51187363 0.48812637]\n"," [0.56510008 0.43489992]\n"," [0.43349989 0.56650011]\n"," [0.57932393 0.42067607]\n"," [0.44974573 0.55025427]\n"," [0.38019652 0.61980348]\n"," [0.59755208 0.40244792]\n"," [0.70731    0.29269   ]\n"," [0.59452469 0.40547531]\n"," [0.55926301 0.44073699]\n"," [0.67339484 0.32660516]\n"," [0.21299064 0.78700936]\n"," [0.48091057 0.51908943]\n"," [0.3337796  0.6662204 ]\n"," [0.49756018 0.50243982]\n"," [0.70863283 0.29136717]\n"," [0.35900545 0.64099455]\n"," [0.58233257 0.41766743]\n"," [0.46589422 0.53410578]\n"," [0.56695753 0.43304247]\n"," [0.52825457 0.47174543]\n"," [0.52801314 0.47198686]\n"," [0.39226834 0.60773166]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d0LWcxbwVz5h"},"source":["For each instance (=row), we get a probability distribution over the classes (=columns)"]},{"cell_type":"markdown","metadata":{"id":"R8QbDmCAVz5h"},"source":["## 6.1 Regularization\n","\n","Typically, performance is lower on unseen data, because our model **overfit** the training data: it expects the new data to look *exactly* the same as the training data. That is almost never true.\n","\n","In order to prevent the model from overfitting, we need to **regularize** it. Essentially, we make it harder to learn the training data.\n","\n","A simple example of regularization is to \"corrupt\" the training data by adding a little bit of noise to each training instance. Since the noise is irregular, it becomes harder for the model to learn any patterns."]},{"cell_type":"code","metadata":{"id":"oO3wiiYMVz5h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617896896781,"user_tz":-120,"elapsed":61467,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"dcf09190-1aec-4da3-8ad5-0c059a2b1e67"},"source":["from scipy.sparse import random\n","\n","num_instances, num_features = X.shape\n","\n","for i in range(5):\n","    X_regularized = X + random(num_instances, num_features, density=0.01)\n","\n","    print(cross_val_score(LogisticRegression(), X_regularized, y=y, cv=k, n_jobs=-1, scoring=\"f1_micro\").mean())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.5227777777777777\n","0.5211111111111111\n","0.5311111111111111\n","0.5283333333333332\n","0.5127777777777778\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ERmjYFKYVz5k"},"source":["If you run the previous cell several times, you see different results (it gets even more varied if you change `density`). This variation arises because we add **random** noise. Not good...\n","\n","Instead, it makes sense to force the model to spread the weights more evenly over all features, rather than bet on a few feature, which might not be present in future data.\n","\n","We can do this by training the model with the `C` parameter. The default is `1`. Lower values mean stricter regularization."]},{"cell_type":"code","metadata":{"id":"l5AUv2tvVz5k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617896917686,"user_tz":-120,"elapsed":17790,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"4c4a752d-aee5-49dc-c753-c4591a494b61"},"source":["from sklearn.metrics import f1_score\n","\n","best_c = None\n","best_f1_score = 0.0\n","\n","for c in [50, 20, 10, 1.0, 0.5, 0.1, 0.05, 0.01]:\n","    clf = LogisticRegression(C=c, n_jobs=-1)\n","    cv_reg = cross_val_score(clf, X, y=y, cv=5, n_jobs=-1, scoring=\"f1_micro\").mean()\n","\n","    print(\"5-CV on train at C={}: {}\".format(c, cv_reg.mean()))\n","    print()\n","\n","    if cv_reg > best_f1_score:\n","        best_f1_score = cv_reg\n","        best_c = c\n","        \n","print(\"best C parameter: {}\".format(best_c))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5-CV on train at C=50: 0.8477777777777777\n","\n","5-CV on train at C=20: 0.8488888888888889\n","\n","5-CV on train at C=10: 0.8488888888888889\n","\n","5-CV on train at C=1.0: 0.828888888888889\n","\n","5-CV on train at C=0.5: 0.8183333333333334\n","\n","5-CV on train at C=0.1: 0.788888888888889\n","\n","5-CV on train at C=0.05: 0.7311111111111112\n","\n","5-CV on train at C=0.01: 0.5077777777777778\n","\n","best C parameter: 20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XA-Q-ENAVz5m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617896918826,"user_tz":-120,"elapsed":18298,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"223bfa99-0ad3-4b58-ad28-690dca60d07f"},"source":["reg_clf = LogisticRegression(C=best_c, n_jobs=-1)\n","reg_clf.fit(X, y)\n","reg_preds = reg_clf.predict(new_X)\n","\n","print(classification_report(new_y, reg_preds))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.86      0.85      0.85       111\n","           1       0.81      0.83      0.82        89\n","\n","    accuracy                           0.84       200\n","   macro avg       0.84      0.84      0.84       200\n","weighted avg       0.84      0.84      0.84       200\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Peq6pHWdVz5n"},"source":["# Better features = better performance\n"]},{"cell_type":"markdown","metadata":{"id":"LTdrezvHVz5o"},"source":["We now have **a lot** of features! More than we have actual examples...\n","\n","Not all of them will be helpful, though. Let's select the top 1500 based on how well they predict they outcome of the training data.\n","\n","We use two libraries from `sklearn`, `SelectKBest` (the selection algorithm) and `chi2` (the selection criterion)."]},{"cell_type":"code","metadata":{"id":"KJr7vjX-Vz5o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617897045177,"user_tz":-120,"elapsed":512,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"e5071c38-a375-4232-fe40-f830bf80d7f4"},"source":["from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","\n","selector = SelectKBest(chi2, k=1500).fit(X, y)\n","X_sel = selector.transform(X)\n","print(X_sel.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1800, 1500)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DY1SU0fLtN1N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617897050395,"user_tz":-120,"elapsed":585,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"1a53d6d1-6eb6-4ec2-a395-9a5622a1affb"},"source":["X.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1800, 66808)"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"markdown","metadata":{"id":"sqHZ06Y0Vz5r"},"source":["Let's see how well this new representation performs, by looking at the 5-fold cross-validation. We keep the best regularization value from before."]},{"cell_type":"code","metadata":{"id":"oDJfQT_dVz5r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617897059431,"user_tz":-120,"elapsed":854,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"cc174b84-ec42-43b5-9630-64e0234c18df"},"source":["clf = LogisticRegression(C=best_c, n_jobs=-1)\n","\n","cv_reg = cross_val_score(clf, X_sel, y=y, cv=5, n_jobs=-1, scoring=\"f1_micro\")\n","print(\"5-CV on train: {}\".format(cv_reg.mean()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5-CV on train: 0.8955555555555555\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O8EzUM5GVz5t"},"source":["Not too bad! We have handily beaten our previous best! Let's fit a classifier on the whole data now."]},{"cell_type":"code","metadata":{"id":"U-3iON0qVz5t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617897061283,"user_tz":-120,"elapsed":501,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"386be3c8-1304-45ea-841a-0268ad39fd22"},"source":["clf.fit(X_sel, y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=20, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=-1, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"markdown","metadata":{"id":"XhE20-xpVz5v"},"source":["Now, let's apply it to the held-out data set. \n","We need to \n","* vectorize the data with our vectorizer from before (otherwise, we get different features)\n","* select the top features (using our previously fitted selector)"]},{"cell_type":"code","metadata":{"id":"dSdElClcVz5v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617897071840,"user_tz":-120,"elapsed":656,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"c3382901-bbf3-473b-bde5-381537d6b474"},"source":["# select features for new data\n","new_X_sel = selector.transform(new_X)\n","print(new_X_sel.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(200, 1500)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V1LNg6z_Vz5x"},"source":["Finally, we can use our new classifier to predict the new data labels, and compare them to the truth."]},{"cell_type":"code","metadata":{"id":"1LzRhzu6Vz5x","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1617897072774,"user_tz":-120,"elapsed":526,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"868506c3-bb85-41d9-a82f-d9eb2d8ee6ff"},"source":["new_predictions_regularized = clf.predict(new_X_sel)\n","prediction_df = pd.DataFrame(data={'input': new_data['input'], 'prediction': labels2numbers.inverse_transform(new_predictions_regularized), 'truth':new_data['output']})\n","prediction_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>prediction</th>\n","      <th>truth</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>robert redford ' s a river runs through it is ...</td>\n","      <td>pos</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>if the 70 ' s nostalgia didn ' t make you feel...</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>you think that these people only exist in the ...</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\" knock off \" is exactly that : a cheap knock ...</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>brian depalma needs a hit * really * badly . s...</td>\n","      <td>pos</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>i won \u0012 t even pretend that i have seen the ot...</td>\n","      <td>pos</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>the cartoon is way better . that ' s the botto...</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>dr . alan grant ( sam neill , \" jurassic park ...</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>of course i knew this going in . why is it tha...</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>rated : r for strong language , sexual dialogu...</td>\n","      <td>pos</td>\n","      <td>pos</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                 input prediction truth\n","0    robert redford ' s a river runs through it is ...        pos   pos\n","1    if the 70 ' s nostalgia didn ' t make you feel...        neg   neg\n","2    you think that these people only exist in the ...        neg   neg\n","3    \" knock off \" is exactly that : a cheap knock ...        neg   neg\n","4    brian depalma needs a hit * really * badly . s...        pos   pos\n","..                                                 ...        ...   ...\n","195  i won \u0012 t even pretend that i have seen the ot...        pos   neg\n","196  the cartoon is way better . that ' s the botto...        neg   neg\n","197  dr . alan grant ( sam neill , \" jurassic park ...        neg   neg\n","198  of course i knew this going in . why is it tha...        neg   neg\n","199  rated : r for strong language , sexual dialogu...        pos   pos\n","\n","[200 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"anoPusQCVz5z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617897073591,"user_tz":-120,"elapsed":626,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"15233f00-8052-4517-9cf5-34e724cad229"},"source":["print(classification_report(new_y, new_predictions_regularized))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.83      0.80      0.82       111\n","           1       0.76      0.80      0.78        89\n","\n","    accuracy                           0.80       200\n","   macro avg       0.80      0.80      0.80       200\n","weighted avg       0.80      0.80      0.80       200\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rA4gUHxOVz50"},"source":["## Getting insights\n","\n","In order to explore which features are most indicative, we need some code"]},{"cell_type":"code","metadata":{"id":"dwMhr1qBVz51","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1617897103057,"user_tz":-120,"elapsed":549,"user":{"displayName":"Debora Nozza","photoUrl":"","userId":"08904684087232240081"}},"outputId":"df8a8bfe-4eed-4419-ac97-2cd18de597ed"},"source":["features = vectorizer.get_feature_names() # get the names of the features\n","top_scores = selector.scores_.argsort()[-1500:] # get the indices of the selection\n","best_indicator_terms = [features[i] for i in sorted(top_scores)] # sort feature names\n","\n","top_indicator_scores = pd.DataFrame(data={'feature': best_indicator_terms, 'coefficient': clf.coef_[0]})\n","top_indicator_scores.sort_values('coefficient')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature</th>\n","      <th>coefficient</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>84</th>\n","      <td>bad</td>\n","      <td>-12.780473</td>\n","    </tr>\n","    <tr>\n","      <th>1449</th>\n","      <td>waste</td>\n","      <td>-8.680148</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>attempt</td>\n","      <td>-8.436183</td>\n","    </tr>\n","    <tr>\n","      <th>1307</th>\n","      <td>suppose</td>\n","      <td>-8.139754</td>\n","    </tr>\n","    <tr>\n","      <th>154</th>\n","      <td>boring</td>\n","      <td>-7.687207</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1004</th>\n","      <td>perfectly</td>\n","      <td>6.340445</td>\n","    </tr>\n","    <tr>\n","      <th>311</th>\n","      <td>definitely</td>\n","      <td>6.399306</td>\n","    </tr>\n","    <tr>\n","      <th>1005</th>\n","      <td>performance</td>\n","      <td>6.453414</td>\n","    </tr>\n","    <tr>\n","      <th>579</th>\n","      <td>hilarious</td>\n","      <td>6.495110</td>\n","    </tr>\n","    <tr>\n","      <th>530</th>\n","      <td>great</td>\n","      <td>9.701571</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1500 rows × 2 columns</p>\n","</div>"],"text/plain":["          feature  coefficient\n","84            bad   -12.780473\n","1449        waste    -8.680148\n","73        attempt    -8.436183\n","1307      suppose    -8.139754\n","154        boring    -7.687207\n","...           ...          ...\n","1004    perfectly     6.340445\n","311    definitely     6.399306\n","1005  performance     6.453414\n","579     hilarious     6.495110\n","530         great     9.701571\n","\n","[1500 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"markdown","metadata":{"id":"K2v3A2Uhf9Qd"},"source":["# Exercise\n","\n","Try to test the model trained on the sentiment analysis dataset on the wine reviews."]},{"cell_type":"code","metadata":{"id":"iXsi_4_Uc_Sq"},"source":["new_data = pd.read_excel('wine_reviews_small.xlsx')\n","print(len(new_data))\n","new_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gPeg8-ac0A0d"},"source":["# Italian classifier\n","\n","In our lab, we developed a Italian emotion and sentiment classifier available at https://github.com/MilaNLProc/feel-it"]},{"cell_type":"code","metadata":{"id":"2hyLrnCM0EDt"},"source":["! pip install -U feel-it"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aj8KXUhJ0AYR"},"source":["from feel_it import EmotionClassifier, SentimentClassifier\n","\n","emotion_classifier = EmotionClassifier()\n","\n","emotion_classifier.predict([\"sono molto felice\", \"ma che cazzo vuoi\", \"sono molto triste\"])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4IuNfSJ0K4m"},"source":["sentiment_classifier = SentimentClassifier()\n","\n","sentiment_classifier.predict([\"sono molto felice\", \"ma che cazzo vuoi\", \"sono molto triste\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o7WMd3Q00zpc"},"source":["# Exercise\n","\n","Download a set of tweets with a specific hashtag in Italian and try to run the Emotion and Sentiment Classifier."]},{"cell_type":"markdown","metadata":{"id":"QgHKC7rTVz54"},"source":["# Checklist: how to classify my data\n","\n","1. label at ***least 2000*** tweets in your data set as `positive`, `negative`, or `neutral`\n","2. preprocess the text of *all* tweets in your data (labeled and unlabeled)\n","3. read in the labeled tweets and their labels\n","4. transform the labels into numbers\n","5. use `TfidfVectorizer` to extract the features and transform them into feature vectors\n","6. select the top $N$ features (where $N$ is smaller than the number of labeled tweets)\n","7. create a classifier\n","8. use 5-fold CV to find the best regularization parameter, top $N$ feature selection, and maybe feature generation and preprocessing steps\n","\n","Once you are satisfied with the results:\n","9. read in the rest of the (unlabeled) tweets\n","10. use the `TfidfVectorizer` from 5. to transform the new data into vectors\n","11. use the `SelectKBest` selector from 6. to get the top $N$ features\n","12. use the classifier from 7. to predict the labels for the new data\n","13. save the predicted labels or probabilities to your database or an Excel file\n"]},{"cell_type":"code","metadata":{"id":"htZQllPWVz54"},"source":[""],"execution_count":null,"outputs":[]}]}