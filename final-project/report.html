<!DOCTYPE html>
<html>
<head>
<title>report.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="text-classification--topic-modelling-on-huffpost-articles">Text classification &amp; Topic modelling on HuffPost articles</h1>
<h2 id="problem">Problem</h2>
<p>The problem we considered is to classify news articles based on their headlines and short descriptions.
Furthermore, since we have a big corpus of articles, we would like to find discussion topics in the corpus, show how they relate to the categories of the dataset and their evolution over time.</p>
<h2 id="dataset">Dataset</h2>
<p>The dataset is composed of around 200k news headlines and summaries from year 2012 to 2018 obtained from HuffPost, the dataset is available at https://www.kaggle.com/datasets/rmisra/news-category-dataset.</p>
<p>The dataset contains, among others, the following columns:</p>
<ul>
<li>category: the category of the article.</li>
<li>headline: the headline of the article.</li>
<li>short_description: the short description of the article.</li>
<li>date: the date the article was published.</li>
<li>full_text: is a derived column by the concatenation of headline and short_description, whenever we use the term article we always refer to this column of the dataset.</li>
</ul>
<p>Below we show the first 5 rows of the dataset.</p>
<p><img src="images/dataset.png" alt=""></p>
<p>Huffpost is an american news aggregator and blog and covers a vast variety of topics including, politics, business, entertainment, tech, lifestyle and others [4].<br>
Rather than using data from a single outlet ex. CNN, MSNBC, Fox, and so on, using articles from a news aggregator should help to have a more balanced view of the discussion topics. However, it must be said that the site &quot;was created to provide a liberal alternative to the conservative news websites such as the Drudge Report&quot; [4]. So it can't be considered to have an unbiased view of the world.</p>
<h2 id="data-cleaning">Data cleaning</h2>
<h3 id="text-cleaning">Text cleaning</h3>
<p>To clean the article text, which is represented by the &quot;full_text&quot; column, we performed the following preprocessing steps in the following order:</p>
<ul>
<li>Lowercase all words: New York --&gt; new york</li>
<li>Replace all numbers with zeros: 2016 --&gt; 0000</li>
<li>Remove Hashtags: #America --&gt; $hashtag (the regex used was <code>#[a-z0-9_]+</code>)</li>
<li>Remove punctuation, in particular <code>!&quot;#&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~</code></li>
<li>Stemming, with SnowballStemmer</li>
<li>Remove stopwords, with list from <code>nltk.stopwords</code></li>
<li>Join bigram collocations, the first 1000 by pmi</li>
</ul>
<p>For the punctuation notice that we didn't remove the symbols <code>%</code> and <code>$</code>, that's because we think those symbols would give insight on the category since the articles also talk about business and finance.</p>
<p>As for the bigram collocations, joining the first 1000 was chosen manually by inspecting the collocations.</p>
<p>Finally, notice that we din't perform lemmatisation that's because spacy preprocessing pipeline, even by disabling everyting apart from the lemmatizer, took about 30ms per document, considering that we have 200k documents this step alone would have taken us 2 hours.</p>
<h3 id="category-merging">Category merging</h3>
<p>The articles, as we said, are classified in 41 categories, however a lot of these categories are duplicates or very similar, for example:</p>
<ul>
<li>ARTS, ARTS &amp; CULTURE, CULTURE &amp; ARTS</li>
<li>WELLNESS, HEALTY LIVING</li>
<li>FOOD &amp; DRINK, TASTE</li>
</ul>
<p>This has happened, among other reasong, because the huffpost has renamed some of the categories during the data collection process.</p>
<p>To solve this problem we decided to merge the following groups of categories into one:</p>
<ul>
<li>HEALTHY LIVING, WELLNESS --&gt; WELLNESS</li>
<li>STYLE, STYLE &amp; BEAUTY --&gt; STYLE &amp; BEAUTY</li>
<li>PARENTS, PARENTING --&gt; PARENTING</li>
<li>TASTE, FOOD &amp; DRINK --&gt; FOOD &amp; DRINK</li>
<li>BUSINESS, BUSINESS &amp; FINANCE --&gt; BUSINESS &amp; FINANCE</li>
<li>MONEY, BUSINESS &amp; FINANCE --&gt; BUSINESS &amp; FINANCE</li>
<li>THE WORLDPOST, WORLD NEWS --&gt; WORLD NEWS</li>
<li>WORLDPOST, WORLD NEWS --&gt; WORLD NEWS</li>
<li>TECH, SCIENCE &amp; TECH --&gt; SCIENCE &amp; TECH</li>
<li>SCIENCE, SCIENCE &amp; TECH --&gt; SCIENCE &amp; TECH</li>
<li>GREEN, ENVIRONMENT --&gt; ENVIRONMENT</li>
<li>CULTURE &amp; ARTS, ARTS &amp; CULTURE --&gt; ARTS &amp; CULTURE</li>
<li>ARTS, ARTS &amp; CULTURE --&gt; ARTS &amp; CULTURE</li>
<li>COLLEGE, EDUCATION --&gt; EDUCATION</li>
</ul>
<p>After the merging process we were left with 29 categories. The class imbalance is still very high though, with politics having 32739 articles while latino voices only has 1129. Below we show a barplot of the 10 most common categories.</p>
<img src="images/top_10_categories.png" alt="drawing" width="500"/>
<h2 id="data-exploration">Data exploration</h2>
<h3 id="t-sne">T-SNE</h3>
<p>We decided to project the data in a 2D space using T-SNE to see how the high-dimensional data distirbution looks like. Prior to using T-SNE we also applied SVD or NMF, however those methods didn't manage to find as much structure in the data as T-SNE did.<br>
The T-SNE was performed on a random subset of 10000 documents belonging in the 10 most common categories, since running it on the whole dataset would have taken too long, and plotting all the categories would have made the plot too cluttered.</p>
<img src="images/tsne+legend.png" alt="drawing" width="550"/>
<p>From the projection we can see that the data is distributed uniformly in a single circular cluster, with a lot of mixing between the classes, this could be caused by the projection of data in a 2D space, or that the headline and the summary of the articles don't provide enough information to determine its category.</p>
<h3 id="wordcloud">WordCloud</h3>
<p>To show the most commonly used terms in the articles and also to summarize the overall topic of discussion we decided to create a WordCloud with the <code>wordcloud</code> python library [5].</p>
<p><img src="images/wordcloud.png" alt=""></p>
<p>We see that politics dominates the debate with words like trump or donald_trump occuring a lot, other relevant words are women, america, travel, family. Finally, we have a lot of generic words that can be found basically everywhere on the internet: good, photo, one, make, show, go, new and so on.<br>
The wordcloud also gives us an insight as to what words to remove next or add to the list of stopwords, furthermore we can spot nasty bugs (i.e. stopwords are removed but the dataframe not updated) in our preprocessing pipeline.</p>
<h2 id="topic-modeling">Topic modeling</h2>
<h3 id="choosing-the-right-number-of-topics">Choosing the right number of topics</h3>
<p>To perform topic modelling we used gensim LDA, we also tried to use sklearn LDA, SVD and NMF but their results were subpar with respect to gensim LDA.<br>
To find the best number of topics we ran gensim LDA on a random subset of 25000 articles, with 20000 used for training and 5000 for evaluation. The evaluation metrics used were u_mass and CV score.<br>
As for the huperparameters we set <code>alpha=eta=1/n_topics</code> which are the default values.</p>
<img src="images/coherence.png" alt="drawing" width="500"/>
<p>We see a spike in both the CV and u_mass scores at 5 topics, so we decided to train an LDA model on the whole corpus using <code>num_topics=5</code>.</p>
<p>The topics of discussion that the model managed to extract from the data are the following:</p>
<img src="images/topic_descriptors.png" alt="drawing" width="400"/>
<p>Topic 1 seems to talk about weddings, topic 2 about trump and us presidents, topic 3 talks about research and health, but also about women, topic 4 talks about hotels and travels, finally topic 5 is very broad, maybe we can associate it to lifestyle or family subjects.</p>
<h3 id="topics-by-category">Topics by category</h3>
<p>To see how the topics relate to the categories of the dataset, we used the learned topic model to assign a topic distribution to each article, then we grouped the articles by category. For presentation purposes we only show the top 5 categories.</p>
<img src="images/topics_by_category.png" alt="drawing" width="450"/>
<p>As we thought topic 5 (make, life) is mostly related to the wellness and parent category, apart from that, the other topics are related to what we expect, for example topic 2 mostly relates to politics and topic 1 to style &amp; beauty.</p>
<h3 id="topics-by-year">Topics by year</h3>
<p>To see the evolution of these topics over time we grouped them by year.</p>
<img src="images/topics_by_year.png" alt="drawing" width="550"/>
<p>We can see that after 2013 the news became more and more politicized with the topic (trump, presid) gaining more and more traction year by year. This obviously is due to the 2016 presedential elections that took place in the US. An interesting fact, however, is that the topic did not peak in 2016, rather it continued to gain attention till 2018, until the data stops. The other topics of discussion seem to decrease over time, we think this is a consequence of topic (trump, presid) gaining more and more interest.<br>
Overall we can say that the discussion in the US has become more and more politicized with time.</p>
<h2 id="text-classification">Text classification</h2>
<p>In the dataset each article is labeled in 40 categories, which are reduced to 29 after the category merging process of the first section. With this information available we decided to classify news articles based on the <code>full_text</code> column, which is the concatenation of the article headline and summary.</p>
<p>First of all we splitted the data into train and test sets as follows:</p>
<ul>
<li>The train set is composed of articles published in 2012 up to 2017</li>
<li>The test set is composed of articles published in 2017 and 2018 and comprises 20% of the data.</li>
</ul>
<p>We think this train-test split is more representative of the use-case of a text classification model, since we want the model to generalize well to articles that will be published in the future.</p>
<p>Whenever we need to set some hyperparameters we use an evaluation set, which we create from a random sample of 20% of the train set.</p>
<p>Finally, to evaluate how a model is performing we will use 4 metrics:</p>
<ol>
<li>F1 weighted: The multiclass F1 score weighted by the number of samples in each class.</li>
<li>F1 macro: The multiclass F1 score un-weighted by the number of samples in each class, i.e. classes with low samples are just as important as classes with a high amount of samples.</li>
<li>Training time: The time to train the whole model.</li>
<li>Evaluation time: The time to predict a single example.</li>
</ol>
<p>The first two metrics tell us the predictive performance of the model, while the second two the amount of compute required in order to get that performance. In particular, the evaluation time is of paramount importance in a production setting, as the computing resources could be very costly to mantain.</p>
<h3 id="naive-bayes">Naive bayes</h3>
<p>The first model that we tried is Multinomial Naive Bayes. The model assumes that all the features are independent from each other, which is a big assumption, however, these models have been used in the past to classify email spam, furthermore they are very fast to train and evaluate [6]. We hope that this simple model will provide a good baseline for further improvement.</p>
<p>Naive bayes requires word counts so we used a bag of words to represent each document. The bag of words was created on the uncleaned text with sklearn <code>CountVectorizer</code> with parameters:</p>
<ul>
<li>lowercase=True</li>
<li>min_df=0.00003</li>
<li>max_df=0.7</li>
<li>strip_accents=&quot;unicode&quot;</li>
<li>ngram_range=(1, 2)</li>
<li>stop_words=&quot;english&quot;</li>
</ul>
<p>The choice to use the uncleaned text rather than the cleaned text was made by looking at the performance on the validation set: the model trained on the uncleaned text managed to get a slight improvement of 0.01 in the F1 scores. However, it must be noted that the text was not completely uncleaned, we simply cleaned it with sklearn <code>CountVectorizer</code> rather than using our pipeline.</p>
<p>The naive bayes model managed to get.</p>
<ul>
<li>F1 weighted: 0.61</li>
<li>F1 macro: 0.36</li>
<li>Training time: 8 s</li>
<li>Evaluation time: 34us/doc</li>
</ul>
<h3 id="logistic-regression">Logistic regression</h3>
<p>In this case we used sklearn <code>TfidfVectorizer</code> with the same parameters as <code>CountVectorizer</code> before. However, the logistic regression classifier has one more hyperparameter <code>C</code> which regulates the amount of L2 regularization applied. To set this parameter we minimized the F1 weighted score on the validation set using Optuna, a bayesian optimization library [2], which found the optimal value to be $C \approx 3.5$ .</p>
<p><img src="images/optuna_C_tuning.png" alt=""></p>
<p>The logistic regression model managed to get.</p>
<ul>
<li>F1 weighted: 0.64</li>
<li>F1 macro: 0.40</li>
<li>Training time: 23 s</li>
<li>Evaluation time: 35 us/doc</li>
</ul>
<h3 id="cnnlstm">CNN/LSTM</h3>
<p>Due to the recent success of Deep Learning approaches to the field of NLP we decided to try a CNN and LSTM model to perform text classification.</p>
<p>In these models we used a custom tokenizer on the <code>full_text_cleaned</code> column of the dataset. The tokenizer applies dynamic padding, which means that it pads all documents to the max length of all the documents in the <strong>batch</strong>, this reduces training times by a lot while also avoiding to truncate sequences.</p>
<p>The CNN model can be seen in the picture below, it has 11M parameters and 256 embedding dimension.</p>
<img src="images/CNN_net.png" alt="drawing" width="350"/>
<p>Even though the model is very complex and takes a very long time to train, however it doesn't manage to get the same performance of logistic regression.</p>
<ul>
<li>F1 weighted: 0.38</li>
<li>F1 macro: 0.58</li>
<li>Training time: 10 min</li>
<li>Evaluation time: 90 us/doc</li>
</ul>
<p>The LSTM model, instead, is composed of 5 stacked LSTMs and has 14M parameters with 256 embedding dimension.</p>
<img src="images/LSTM_net.png" alt="drawing" width="350"/>
<p>As with the CNN model, even though the model is very complex it doesn't manage to surpass logistic regression.</p>
<ul>
<li>F1 weighted: 0.37</li>
<li>F1 macro: 0.58</li>
<li>Training time: 14 min</li>
<li>Evaluation time: 130 us/doc</li>
</ul>
<p>It must be noted that in both the CNN and LSTM models train and evaluation time are computed on a Nvidia T4 GPU, training and evaluating them on CPU would have taken at least 10 times more.</p>
<h3 id="transformer">Transformer</h3>
<p>Finally, we tried to use a pretrained transformer to classify the articles, the specific model used is distilbert-base-uncased which is a distillation of bert-base-uncased. The model, with respect to bert-base-uncased is:</p>
<ul>
<li>More parameter efficent, 40% less parameters</li>
<li>60% faster</li>
<li>Preserves over 95% of performances in the GLUE benchmark [3]</li>
</ul>
<p>The model was trained for one epoch using the huggingface transformer api, following distilbert-base-uncased <a href="https://huggingface.co/docs/transformers/tasks/sequence_classification">fine tuning guide</a>.
The final performance is a major improvement in the F1 scores on all the previous models, at the cost of way more compute time on GPU.</p>
<ul>
<li>F1 weighted: 0.45</li>
<li>F1 macro: 0.68</li>
<li>Training time: 20 min</li>
<li>Evaluation time: 1600 us/doc</li>
</ul>
<h3 id="results">Results</h3>
<p>To summarize all the results of the text classification experiments, we used two bar plots, as we think the reader can appreciate more the difference in performance this way, rather than displaying them in a table.</p>
<img src="images/metrics.png" alt="drawing" width="500"/>
<img src="images/times.png" alt="drawing" width="500"/>
<p>We can say that Naive bayes is very fast to train, evaluate and provides decent results. Logistic regression is better than naive bayes in any metric apart from the training time, which takes about 3 times more.</p>
<p>As for the neural network models CNN and LSTM are very slow to train and evaluate, on top of that they have worse F1 scores than logistic regression. We can confidently say that in any task and any situation a logistic regression model would be a better choice than any of these models.</p>
<p>Finally, the transformer is the model that performs best in terms of F1 scores, this comes at the cost of huge training and evaluation times with respect to logistic regression or naive bayes. A use case of this model could be when we need accurate predictions or when computing resources are not a problem.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We took a dataset of 200k Huffpost articles and found the topics of discussion using gensim topic model. By analyzing the popularity of the topics over time we demonstrated how the US news have become more and more politicized, with the topic talking about the US president donald trump becoming the main subject of discussion over the years and still growing in 2018, where the data stops.</p>
<p>We also trained text classification models to classify a news article based on its headline and summary, the goodness of a model was measured not only using F1 scores, but also with training and evaluation time, as we think they are of paramount importance in a production scenario. The best model in terms of F1 scores was found by fine-tuning the pretrained distilbert-base-uncased transformer model, this model manages to get a F1_weighted score of 0.68 at the cost of a high evaluation and training time. A simpler and lighter model we considered is logistic regression, which takes less that 1/100 to train and evaluate w.r.t. the transformer model with only a decrease of 3% in F1 score.</p>
<h2 id="references">References</h2>
<ol>
<li>News category dataset. https://www.kaggle.com/datasets/rmisra/news-category-dataset</li>
<li>Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In KDD.</li>
<li>distilbert-base-uncased. https://huggingface.co/docs/transformers/model_doc/distilbert</li>
<li>Huffpost on Wikipedia. https://en.wikipedia.org/wiki/HuffPost</li>
<li>WordCloud python library. https://github.com/amueller/word_cloud</li>
<li>Naive Bayes spam filtering. https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering</li>
</ol>

</body>
</html>
